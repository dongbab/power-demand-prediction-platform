# ğŸ› ï¸ ìœ ì§€ë³´ìˆ˜ ê°€ì´ë“œ

> EV ì¶©ì „ì†Œ ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡ í”Œë«í¼ ìœ ì§€ë³´ìˆ˜ ë§¤ë‰´ì–¼

## ğŸ“‹ ëª©ì°¨
- [ì‹œìŠ¤í…œ ê°œìš”](#ì‹œìŠ¤í…œ-ê°œìš”)
- [ê°œë°œ í™˜ê²½ ì„¤ì •](#ê°œë°œ-í™˜ê²½-ì„¤ì •)
- [ì¼ìƒ ìœ ì§€ë³´ìˆ˜](#ì¼ìƒ-ìœ ì§€ë³´ìˆ˜)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)
- [ë°°í¬ ê´€ë¦¬](#ë°°í¬-ê´€ë¦¬)
- [ëª¨ë‹ˆí„°ë§](#ëª¨ë‹ˆí„°ë§)
- [ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬](#ë°ì´í„°ë² ì´ìŠ¤-ê´€ë¦¬)
- [ë°±ì—… ë° ë³µêµ¬](#ë°±ì—…-ë°-ë³µêµ¬)

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ê°œìš”

### ì•„í‚¤í…ì²˜
```
Frontend (SvelteKit)  â†â†’  Backend (FastAPI)  â†â†’  ë°ì´í„° (CSV/DB)
     â†“                        â†“                      â†“
  - ëŒ€ì‹œë³´ë“œ               - REST API              - ì¶©ì „ ì´ë ¥
  - ì°¨íŠ¸/ê·¸ë˜í”„            - ì˜ˆì¸¡ ì—”ì§„             - ìºì‹œ ì‹œìŠ¤í…œ
  - íŒŒì¼ ì—…ë¡œë“œ            - ë°ì´í„° ì²˜ë¦¬           - ë¡œê·¸ íŒŒì¼
```

### ê¸°ìˆ  ìŠ¤íƒ
**Backend**
- FastAPI (Python ì›¹ í”„ë ˆì„ì›Œí¬)
- Pandas (ë°ì´í„° ë¶„ì„)
- NumPy (ìˆ˜ì¹˜ ê³„ì‚°)
- SciPy (ê³ ê¸‰ í†µê³„ ë° ê·¹ê°’ ì´ë¡ )
- Uvicorn (ASGI ì„œë²„)

**Frontend**  
- SvelteKit (ì›¹ í”„ë ˆì„ì›Œí¬)
- Chart.js (ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬)
- TypeScript (íƒ€ì… ì•ˆì •ì„±)
- Tailwind CSS (ìŠ¤íƒ€ì¼ë§)

---

## ğŸ› ï¸ ê°œë°œ í™˜ê²½ ì„¤ì •

### 1. í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜
```bash
# Python 3.8+ í™•ì¸
python --version

# Node.js 18+ í™•ì¸
node --version

# Git ì„¤ì¹˜ í™•ì¸
git --version
```

### 2. í”„ë¡œì íŠ¸ í´ë¡  ë° ì„¤ì •
```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
git clone <repository-url>
cd power-demand-prediciton-platform

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ í¸ì§‘í•˜ì—¬ í•„ìš”í•œ ì„¤ì • ë³€ê²½
```

### 3. ë°±ì—”ë“œ í™˜ê²½ ì„¤ì •
```bash
cd backend

# ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ê°œë°œ ì„œë²„ ì‹¤í–‰
uvicorn app.main:app --reload --port 8000
```

### 4. í”„ë¡ íŠ¸ì—”ë“œ í™˜ê²½ ì„¤ì •
```bash
cd frontend

# ì˜ì¡´ì„± ì„¤ì¹˜
npm install

# ê°œë°œ ì„œë²„ ì‹¤í–‰
npm run dev
```

### 5. ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰ (Docker)
```bash
# ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f
```

---

## ğŸ”„ ì¼ìƒ ìœ ì§€ë³´ìˆ˜

### ë§¤ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### 1. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
```bash
# ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:8000/health
curl http://localhost:8000/api/status

# ë¡œê·¸ í™•ì¸
tail -f logs/main.log
grep ERROR logs/main.log | tail -20
```

#### 2. ë°ì´í„° ìƒíƒœ í™•ì¸
```bash
# ë°ì´í„° ì—…ë¡œë“œ ìƒíƒœ í™•ì¸
ls -la data/raw/
du -h data/raw/  # ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸

# ë°ì´í„° í’ˆì§ˆ í™•ì¸
python debug_tool.py --check-data
```

#### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```bash
# CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
top -p $(pgrep -f "uvicorn")

# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
df -h

# ë¡œê·¸ íŒŒì¼ í¬ê¸° í™•ì¸
du -h logs/
```

### ì£¼ê°„ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### 1. ë¡œê·¸ ê´€ë¦¬
```bash
# ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì••ì¶•
gzip logs/main.log.$(date -d "7 days ago" +%Y%m%d)

# ë¡œê·¸ íŒŒì¼ í¬ê¸° ì œí•œ (100MB ì´ˆê³¼ ì‹œ)
if [ $(stat -f%z logs/main.log) -gt 104857600 ]; then
  cp logs/main.log logs/main.log.backup
  > logs/main.log
fi
```

#### 2. ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
```bash
# CSV íŒŒì¼ ì •ë¦¬
find data/raw/ -name "*.csv.bak" -mtime +30 -delete

# ì„ì‹œ íŒŒì¼ ì •ë¦¬
find . -name "*.tmp" -delete
find . -name "__pycache__" -type d -exec rm -rf {} +
```

#### 3. ë³´ì•ˆ ì—…ë°ì´íŠ¸ í™•ì¸
```bash
# Python íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸ í™•ì¸
pip list --outdated

# Node.js íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸ í™•ì¸
cd frontend && npm outdated
```

### ì›”ê°„ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### 1. ë°±ì—… í™•ì¸
```bash
# ë°ì´í„° ë°±ì—… ìƒì„±
tar -czf backup/data_backup_$(date +%Y%m%d).tar.gz data/

# ì„¤ì • íŒŒì¼ ë°±ì—…
cp .env backup/env_backup_$(date +%Y%m%d)
cp docker-compose.yml backup/
```

#### 2. ì„±ëŠ¥ ë¶„ì„
```bash
# ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
python debug_tool.py --performance-report

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì´ ë¶„ì„
grep "Memory usage" logs/main.log | tail -100 > reports/memory_usage.log
```

---

## ğŸš¨ ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

#### 1. ì„œë²„ ì‹œì‘ ì‹¤íŒ¨
**ì¦ìƒ**: `Address already in use` ì˜¤ë¥˜

**í•´ê²° ë°©ë²•**:
```bash
# í¬íŠ¸ ì‚¬ìš© í”„ë¡œì„¸ìŠ¤ í™•ì¸
lsof -i :8000
# ë˜ëŠ” Windowsì—ì„œ
netstat -ano | findstr :8000

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
kill -9 <PID>

# ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
uvicorn app.main:app --port 8001
```

#### 2. CSV íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜
**ì¦ìƒ**: "CSV íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ì˜¤ë¥˜

**í•´ê²° ë°©ë²•**:
```bash
# 1. íŒŒì¼ ê¶Œí•œ í™•ì¸
ls -la data/raw/

# 2. íŒŒì¼ ì¸ì½”ë”© í™•ì¸
file -bi data/raw/*.csv

# 3. CSV íŒŒì¼ êµ¬ì¡° í™•ì¸
head -5 data/raw/charging_data.csv

# 4. ë°ì´í„° ê²€ì¦ ë„êµ¬ ì‹¤í–‰
python -c "
import pandas as pd
df = pd.read_csv('data/raw/charging_data.csv')
print(f'ì»¬ëŸ¼: {df.columns.tolist()}')
print(f'í–‰ ìˆ˜: {len(df)}')
print(df.head())
"
```

#### 3. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
**ì¦ìƒ**: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ ì‹œìŠ¤í…œ ëŠë ¤ì§

**í•´ê²° ë°©ë²•**:
```bash
# 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
free -h
ps aux --sort=-%mem | head -10

# 2. í™˜ê²½ ë³€ìˆ˜ ì¡°ì •
echo "MAX_SESSIONS_PER_QUERY=5000" >> .env
echo "CHUNK_SIZE=10000" >> .env

# 3. ì„œë²„ ì¬ì‹œì‘
docker-compose restart backend
```

#### 4. ì˜ˆì¸¡ ê²°ê³¼ ì´ìƒ
**ì¦ìƒ**: ì˜ˆì¸¡ê°’ì´ í˜„ì‹¤ì ì´ì§€ ì•Šê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ

**í•´ê²° ë°©ë²•**:
```bash
# 1. ë°ì´í„° í’ˆì§ˆ í™•ì¸
python debug_station_data.py <STATION_ID>

# 2. ì˜ˆì¸¡ ëª¨ë¸ ë””ë²„ê¹…
python debug_contract_prediction.py <STATION_ID>

# 3. í†µê³„ ë¶„ì„ í™•ì¸
python -c "
from app.services.station_service import StationService
service = StationService()
stats = service.get_station_statistics('<STATION_ID>')
print(stats)
"
```

### ê¸´ê¸‰ ìƒí™© ëŒ€ì‘

#### ì‹œìŠ¤í…œ ì „ì²´ ì¤‘ë‹¨
```bash
# 1. ì¦‰ì‹œ ì„œë¹„ìŠ¤ ì¤‘ë‹¨
docker-compose down

# 2. ë¡œê·¸ ë¶„ì„
tail -100 logs/main.log
grep "CRITICAL\|ERROR" logs/main.log

# 3. ë°±ì—…ì—ì„œ ë³µêµ¬
cp backup/env_backup_latest .env
tar -xzf backup/data_backup_latest.tar.gz

# 4. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose up -d
```

#### ë°ì´í„° ì†ì‹¤ ë°œìƒ
```bash
# 1. ì„œë¹„ìŠ¤ ì¦‰ì‹œ ì¤‘ë‹¨
docker-compose down

# 2. ë°ì´í„° ìƒíƒœ í™•ì¸
ls -la data/raw/
find data/ -name "*.csv" -mtime -1

# 3. ë°±ì—…ì—ì„œ ë³µêµ¬
tar -xzf backup/data_backup_$(date -d "yesterday" +%Y%m%d).tar.gz

# 4. ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
python debug_tool.py --verify-data
```

---

## ğŸš€ ë°°í¬ ê´€ë¦¬

### ê°œë°œ í™˜ê²½ ë°°í¬
```bash
# ì½”ë“œ ì—…ë°ì´íŠ¸
git pull origin main

# ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
cd backend && pip install -r requirements.txt
cd frontend && npm install

# ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose restart
```

### í”„ë¡œë•ì…˜ ë°°í¬

#### ë°°í¬ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] í…ŒìŠ¤íŠ¸ í†µê³¼ í™•ì¸
- [ ] ë°±ì—… ì™„ë£Œ
- [ ] ì„¤ì • íŒŒì¼ í™•ì¸
- [ ] ë³´ì•ˆ ì„¤ì • ì ê²€

#### ë°°í¬ ì ˆì°¨
```bash
# 1. ë°±ì—… ìƒì„±
./scripts/backup.sh

# 2. ì½”ë“œ ë°°í¬
git pull origin main

# 3. ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
pip install -r requirements.txt --no-deps

# 4. ë§ˆì´ê·¸ë ˆì´ì…˜ (í•„ìš”ì‹œ)
python scripts/migrate_data.py

# 5. ì„œë¹„ìŠ¤ ì¬ì‹œì‘ (ë¬´ì¤‘ë‹¨ ë°°í¬)
docker-compose up -d --no-deps backend

# 6. í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/health
```

### ë¡¤ë°± ì ˆì°¨
```bash
# 1. ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
git reset --hard <PREVIOUS_COMMIT>

# 2. ë°ì´í„° ë³µêµ¬
tar -xzf backup/data_backup_before_deploy.tar.gz

# 3. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose restart

# 4. ìƒíƒœ í™•ì¸
curl http://localhost:8000/health
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

#### CPU/ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
```bash
# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > monitor.sh << 'EOF'
#!/bin/bash
while true; do
  echo "$(date): CPU=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1) Memory=$(free -m | awk 'NR==2{printf "%.1f%%", $3*100/$2}')" >> logs/system_metrics.log
  sleep 60
done
EOF

chmod +x monitor.sh
nohup ./monitor.sh &
```

#### API ì‘ë‹µ ì‹œê°„ ëª¨ë‹ˆí„°ë§
```bash
# API ì‘ë‹µ ì‹œê°„ ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
cat > api_monitor.sh << 'EOF'
#!/bin/bash
for endpoint in "/health" "/api/status" "/api/stations"; do
  response_time=$(curl -o /dev/null -s -w "%{time_total}" http://localhost:8000$endpoint)
  echo "$(date): $endpoint - ${response_time}s" >> logs/api_metrics.log
done
EOF

chmod +x api_monitor.sh
# í¬ë¡ íƒ­ì— ë“±ë¡: */5 * * * * /path/to/api_monitor.sh
```

### ë¡œê·¸ ë¶„ì„

#### ì—ëŸ¬ ë¡œê·¸ ë¶„ì„
```bash
# ìµœê·¼ 24ì‹œê°„ ì—ëŸ¬ í†µê³„
grep "$(date -d '1 day ago' '+%Y-%m-%d')" logs/main.log | grep ERROR | wc -l

# ìì£¼ ë°œìƒí•˜ëŠ” ì—ëŸ¬ TOP 10
grep ERROR logs/main.log | awk -F': ' '{print $NF}' | sort | uniq -c | sort -nr | head -10

# íŠ¹ì • ê¸°ê°„ ì„±ëŠ¥ ë¶„ì„
awk '/2024-01-01/,/2024-01-02/ {print}' logs/main.log | grep "response_time" > performance_analysis.log
```

#### ì‚¬ìš©ì í™œë™ ë¶„ì„
```bash
# API ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© í†µê³„
grep "GET\|POST" logs/main.log | awk '{print $7}' | sort | uniq -c | sort -nr

# ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½ ë¶„ì„
grep "$(date '+%Y-%m-%d')" logs/main.log | cut -d' ' -f2 | cut -d':' -f1 | sort | uniq -c
```

---

## ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬

### CSV íŒŒì¼ ê´€ë¦¬

#### ë°ì´í„° ì •í•©ì„± ê²€ì¦
```bash
# CSV íŒŒì¼ êµ¬ì¡° ê²€ì¦
python -c "
import pandas as pd
import os

data_dir = 'data/raw'
required_columns = ['ì¶©ì „ì†ŒID', 'ì¶©ì „ì‹œì‘ì¼ì‹œ', 'ìˆœê°„ìµœê³ ì „ë ¥']

for file in os.listdir(data_dir):
    if file.endswith('.csv'):
        try:
            df = pd.read_csv(os.path.join(data_dir, file))
            missing_cols = [col for col in required_columns if col not in df.columns]
            if missing_cols:
                print(f'{file}: ëˆ„ë½ëœ ì»¬ëŸ¼ - {missing_cols}')
            else:
                print(f'{file}: ì •ìƒ (ì´ {len(df)}í–‰)')
        except Exception as e:
            print(f'{file}: ì˜¤ë¥˜ - {e}')
"
```

#### ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬
```bash
# ì¤‘ë³µ ë°ì´í„° ì œê±° ìŠ¤í¬ë¦½íŠ¸
python -c "
import pandas as pd
import glob

for file_path in glob.glob('data/raw/*.csv'):
    df = pd.read_csv(file_path)
    original_count = len(df)
    df_clean = df.drop_duplicates()
    clean_count = len(df_clean)
    
    if original_count != clean_count:
        print(f'{file_path}: {original_count} â†’ {clean_count} ({original_count - clean_count}ê°œ ì¤‘ë³µ ì œê±°)')
        df_clean.to_csv(file_path, index=False)
    else:
        print(f'{file_path}: ì¤‘ë³µ ì—†ìŒ')
"
```

### ë°ì´í„° ì•„ì¹´ì´ë¸Œ

#### ì˜¤ë˜ëœ ë°ì´í„° ì•„ì¹´ì´ë¸Œ
```bash
# 30ì¼ ì´ìƒ ëœ CSV íŒŒì¼ ì•„ì¹´ì´ë¸Œ
find data/raw/ -name "*.csv" -mtime +30 -exec mv {} archive/ \;

# ì›”ë³„ ì•„ì¹´ì´ë¸Œ ìƒì„±
for month in {01..12}; do
  tar -czf "archive/2024_${month}_data.tar.gz" data/raw/*2024-${month}*.csv
done
```

---

## ğŸ’¾ ë°±ì—… ë° ë³µêµ¬

### ìë™ ë°±ì—… ì„¤ì •

#### ì¼ì¼ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
```bash
cat > scripts/daily_backup.sh << 'EOF'
#!/bin/bash

BACKUP_DIR="backup/$(date +%Y%m%d)"
mkdir -p "$BACKUP_DIR"

# ë°ì´í„° ë°±ì—…
echo "ë°ì´í„° ë°±ì—… ì‹œì‘..."
tar -czf "$BACKUP_DIR/data_backup.tar.gz" data/

# ì„¤ì • íŒŒì¼ ë°±ì—…
echo "ì„¤ì • íŒŒì¼ ë°±ì—…..."
cp .env "$BACKUP_DIR/"
cp docker-compose.yml "$BACKUP_DIR/"

# ë¡œê·¸ ë°±ì—…
echo "ë¡œê·¸ ë°±ì—…..."
cp -r logs/ "$BACKUP_DIR/"

# ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬ (30ì¼ ì´ì „)
find backup/ -type d -mtime +30 -exec rm -rf {} \;

echo "ë°±ì—… ì™„ë£Œ: $BACKUP_DIR"
EOF

chmod +x scripts/daily_backup.sh

# í¬ë¡ íƒ­ ë“±ë¡ (ë§¤ì¼ ì˜¤ì „ 2ì‹œ)
echo "0 2 * * * /path/to/scripts/daily_backup.sh" | crontab -
```

### ë³µêµ¬ ì ˆì°¨

#### ë°ì´í„° ë³µêµ¬
```bash
# 1. ì„œë¹„ìŠ¤ ì¤‘ë‹¨
docker-compose down

# 2. í˜„ì¬ ë°ì´í„° ë°±ì—… (ì•ˆì „ì¥ì¹˜)
mv data/ data_corrupted_$(date +%Y%m%d)/

# 3. ë°±ì—…ì—ì„œ ë³µêµ¬
tar -xzf backup/20240115/data_backup.tar.gz

# 4. ê¶Œí•œ ì„¤ì •
chown -R $(whoami):$(whoami) data/
chmod -R 755 data/

# 5. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose up -d

# 6. ë³µêµ¬ í™•ì¸
python debug_tool.py --verify-data
```

#### ì„¤ì • íŒŒì¼ ë³µêµ¬
```bash
# ì„¤ì • íŒŒì¼ ë³µêµ¬
cp backup/20240115/.env .
cp backup/20240115/docker-compose.yml .

# í™˜ê²½ ë³€ìˆ˜ ë‹¤ì‹œ ë¡œë“œ
docker-compose down
docker-compose up -d
```

---

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### ì„±ëŠ¥ íŠœë‹

#### FastAPI ì„œë²„ ìµœì í™”
```python
# app/main.py ì„±ëŠ¥ ì„¤ì • ì¶”ê°€
from fastapi import FastAPI
import asyncio
import uvloop

# ì´ë²¤íŠ¸ ë£¨í”„ ìµœì í™”
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

app = FastAPI(
    title="Power Demand Prediction Platform",
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    generate_unique_id_function=lambda route: f"{route.tags[0]}-{route.name}",
)

# ë¯¸ë“¤ì›¨ì–´ ìµœì í™”
@app.middleware("http")
async def add_performance_headers(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response
```

#### ë°ì´í„° ì²˜ë¦¬ ìµœì í™”
```python
# ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬ ì„¤ì •
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 10000))
MAX_MEMORY_USAGE = int(os.getenv("MAX_MEMORY_MB", 1024))

# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ CSV ì½ê¸°
def read_large_csv(file_path):
    return pd.read_csv(
        file_path,
        chunksize=CHUNK_SIZE,
        dtype={'ì¶©ì „ì†ŒID': 'category', 'ìˆœê°„ìµœê³ ì „ë ¥': 'float32'},
        parse_dates=['ì¶©ì „ì‹œì‘ì¼ì‹œ']
    )
```

### ë³´ì•ˆ ì„¤ì •

#### API ë³´ì•ˆ ê°•í™”
```python
# í™˜ê²½ë³€ìˆ˜ë¡œ API í‚¤ ì„¤ì •
API_SECRET_KEY = os.getenv("API_SECRET_KEY", "change-this-secret-key")

# Rate Limiting ì¶”ê°€
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.get("/api/predict/{station_id}")
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ
async def predict_power_demand(request: Request, station_id: str):
    # ì˜ˆì¸¡ ë¡œì§
    pass
```

---

### ìœ ìš©í•œ ëª…ë ¹ì–´ ëª¨ìŒ
```bash
# ë¹ ë¥¸ ìƒíƒœ í™•ì¸
curl http://localhost:8000/health && echo "âœ… Backend OK"
curl http://localhost:5173 && echo "âœ… Frontend OK"

# ë¡œê·¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
tail -f logs/main.log | grep -E "(ERROR|WARNING|INFO)"

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
watch -n 1 'free -h && echo "---" && ps aux --sort=-%mem | head -10'

# í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘ (ìš°ì•„í•œ ì¢…ë£Œ)
pkill -TERM -f "uvicorn"
sleep 5
uvicorn app.main:app --host 0.0.0.0 --port 8000 &
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸ ìš”ì•½

### ì„¤ì¹˜ í›„ ì²« ì„¤ì • ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Python 3.8+ ì„¤ì¹˜ í™•ì¸
- [ ] Node.js 18+ ì„¤ì¹˜ í™•ì¸  
- [ ] í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ
- [ ] í™˜ê²½ ë³€ìˆ˜(.env) ì„¤ì • ì™„ë£Œ
- [ ] ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„± ë° ê¶Œí•œ ì„¤ì •
- [ ] ì²« í…ŒìŠ¤íŠ¸ CSV íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬ í™•ì¸
- [ ] API ì—”ë“œí¬ì¸íŠ¸ ë™ì‘ í™•ì¸
- [ ] í”„ë¡ íŠ¸ì—”ë“œ í˜ì´ì§€ ì ‘ê·¼ í™•ì¸

### ì •ê¸° ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸  
- [ ] ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸ (CPU, ë©”ëª¨ë¦¬, ë””ìŠ¤í¬)
- [ ] ë¡œê·¸ íŒŒì¼ í¬ê¸° ë° ì—ëŸ¬ ë°œìƒ ì—¬ë¶€ í™•ì¸
- [ ] ë°±ì—… íŒŒì¼ ìƒì„± ë° ë¬´ê²°ì„± í™•ì¸
- [ ] ë³´ì•ˆ ì—…ë°ì´íŠ¸ í™•ì¸ ë° ì ìš©
- [ ] ì„±ëŠ¥ ì§€í‘œ ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ë° ì´ìŠˆ ì‚¬í•­ ì ê²€

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### 1. **ì˜ˆì¸¡ ì—”ì§„ ì„±ëŠ¥ ë¶„ì„**

#### í˜„ì¬ ì„±ëŠ¥ ì§€í‘œ
- **í‰ê·  ì‘ë‹µ ì‹œê°„**: 1-2ì´ˆ (8ê°œ ëª¨ë¸ ìˆœì°¨ ì‹¤í–‰)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 50-100MB (ë¶€íŠ¸ìŠ¤íŠ¸ë© ë° ë°ì´í„° ë³µì‚¬ë³¸)
- **CPU ì‚¬ìš©ë¥ **: ë‹¨ì¼ ì½”ì–´ ì§‘ì¤‘ ì‚¬ìš©

#### ë³‘ëª© ì§€ì  ì‹ë³„
```bash
# ì˜ˆì¸¡ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
python -m cProfile -o profile_stats.prof -c "
from app.prediction.engine import PredictionEngine
import pandas as pd
engine = PredictionEngine()
data = pd.read_csv('data/sample_station.csv')
engine.predict_contract_power(data, 'TEST001')
"

# í”„ë¡œíŒŒì¼ ê²°ê³¼ ë¶„ì„
python -c "
import pstats
stats = pstats.Stats('profile_stats.prof')
stats.sort_stats('cumulative').print_stats(20)
"
```

### 2. **ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”**

#### ThreadPoolExecutor ì„¤ì •
```python
# engine.pyì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ êµ¬ì„±
max_workers = min(4, os.cpu_count())  # CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜ ì„¤ì •
timeout = 10  # ëª¨ë¸ë³„ ìµœëŒ€ ì‹¤í–‰ ì‹œê°„

# ëª¨ë¸ ê·¸ë£¹ë³„ ë³‘ë ¬ ì‹¤í–‰
- ê·¹ê°’ ì´ë¡  ëª¨ë¸ (EVT)
- í†µê³„ì  ì¶”ë¡  ëª¨ë¸ (STAT)  
- ì‹œê³„ì—´ ëª¨ë¸ (TS)
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ (ML)
```

#### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
```bash
# ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ ì¸¡ì •
cat > benchmark_parallel.py << 'EOF'
import time
import pandas as pd
from app.prediction.engine import PredictionEngine

def benchmark_prediction(station_id, iterations=10):
    engine = PredictionEngine()
    data = pd.read_csv(f'data/stations/{station_id}.csv')
    
    times = []
    for i in range(iterations):
        start = time.time()
        result = engine.predict_contract_power(data, station_id)
        elapsed = time.time() - start
        times.append(elapsed)
        print(f"Iteration {i+1}: {elapsed:.3f}s, Models: {len(result.model_predictions)}")
    
    avg_time = sum(times) / len(times)
    print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.3f}s")
    return avg_time

if __name__ == "__main__":
    benchmark_prediction("STATION_001")
EOF

python benchmark_parallel.py
```

### 3. **ìºì‹± ì „ëµ**

#### í†µê³„ëŸ‰ ìºì‹±
```python
# ê¸°ë³¸ í†µê³„ëŸ‰ ì‚¬ì „ ê³„ì‚° ë° ìºì‹±
stats_cache = {
    'mean', 'median', 'std', 'mad',
    'q25', 'q75', 'q90', 'q95', 'q99',
    'min', 'max'
}

# LRU ìºì‹œ í¬ê¸°: 128ê°œ ë°ì´í„°ì…‹
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~10MB (ì¼ë°˜ì ì¸ ì¶©ì „ì†Œ ë°ì´í„° ê¸°ì¤€)
```

#### ìºì‹œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```bash
# ìºì‹œ íˆíŠ¸ìœ¨ í™•ì¸
python -c "
from app.prediction.engine import PredictionEngine
engine = PredictionEngine()
print('ìºì‹œ ì •ë³´:', engine._stats_cache.cache_info())
"

# ìºì‹œ í´ë¦¬ì–´ (í•„ìš”ì‹œ)
curl -X POST http://localhost:8000/api/admin/clear-cache
```

### 4. **ë©”ëª¨ë¦¬ ìµœì í™”**

#### ë¶€íŠ¸ìŠ¤íŠ¸ë© ìµœì í™”
- **ê¸°ì¡´**: 1000íšŒ ë°˜ë³µ, ìˆœì°¨ ì²˜ë¦¬
- **ê°œì„ **: 200íšŒ ë°˜ë³µ, ë²¡í„°í™” ì—°ì‚°
- **ë©”ëª¨ë¦¬ ì ˆì•½**: ~60% ê°ì†Œ

#### ë°ì´í„° ì¬ì‚¬ìš©
```python
# ë¶ˆí•„ìš”í•œ ë°°ì—´ ë³µì‚¬ ì œê±°
# í†µê³„ëŸ‰ ì¤‘ë³µ ê³„ì‚° ë°©ì§€
# íˆìŠ¤í† ê·¸ë¨ bins ìˆ˜ ê°ì†Œ (30 â†’ 20)
```

### 5. **ì„±ëŠ¥ ì¸¡ì • ë° ëª¨ë‹ˆí„°ë§**

#### ì‹¤ì‹œê°„ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ
```bash
# FastAPI ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
pip install prometheus-fastapi-instrumentator

# Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •
curl http://localhost:3000/api/dashboards/db \
  -X POST \
  -H "Content-Type: application/json" \
  -d @monitoring/prediction_performance.json
```

#### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
```bash
# ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
cat > load_test.py << 'EOF'
import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor

async def test_prediction_endpoint(session, station_id):
    url = f"http://localhost:8000/api/stations/{station_id}/predict"
    start = time.time()
    async with session.get(url) as response:
        await response.json()
        return time.time() - start

async def run_load_test(concurrent_requests=10, total_requests=100):
    async with aiohttp.ClientSession() as session:
        tasks = []
        for i in range(total_requests):
            station_id = f"STATION_{i % 5 + 1:03d}"
            task = test_prediction_endpoint(session, station_id)
            tasks.append(task)
            
        results = await asyncio.gather(*tasks)
        
    avg_time = sum(results) / len(results)
    print(f"ë™ì‹œ ìš”ì²­ {concurrent_requests}ê°œ, ì´ {total_requests}ê°œ")
    print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.3f}s")
    print(f"ìµœëŒ€ ì‘ë‹µ ì‹œê°„: {max(results):.3f}s")
    print(f"ìµœì†Œ ì‘ë‹µ ì‹œê°„: {min(results):.3f}s")

if __name__ == "__main__":
    asyncio.run(run_load_test())
EOF

python load_test.py
```

### 6. **ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸**

#### ì¼ì¼ ì ê²€ ì‚¬í•­
```bash
# 1. í‰ê·  ì‘ë‹µ ì‹œê°„ í™•ì¸ (ëª©í‘œ: <0.5ì´ˆ)
curl -w "Time: %{time_total}s\n" http://localhost:8000/api/stations/STATION_001/predict

# 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
ps aux | grep "python.*main.py" | awk '{print $4 "% " $6/1024 "MB"}'

# 3. ìºì‹œ íš¨ìœ¨ì„± ì ê²€
python -c "
from app.prediction.engine import PredictionEngine
engine = PredictionEngine()
print('ìºì‹œ ìƒíƒœ:', len(engine._stats_cache), 'ê°œ í•­ëª©')
"
```

#### ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì •
- **ì‘ë‹µ ì‹œê°„**: 
  - ì–‘í˜¸: <0.5ì´ˆ
  - ì£¼ì˜: 0.5-1.0ì´ˆ  
  - ê²½ê³ : >1.0ì´ˆ
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**:
  - ì–‘í˜¸: <100MB
  - ì£¼ì˜: 100-200MB
  - ê²½ê³ : >200MB
- **ìºì‹œ íˆíŠ¸ìœ¨**:
  - ì–‘í˜¸: >80%
  - ì£¼ì˜: 60-80%
  - ê²½ê³ : <60%

### 7. **ì„±ëŠ¥ ë¬¸ì œ í•´ê²°**

#### ì‘ë‹µ ì‹œê°„ ì§€ì—° ì‹œ
```bash
# 1. ë³‘ëª© ì§€ì  ë¶„ì„
python -m line_profiler prediction_profile.py

# 2. ë¬´ê±°ìš´ ëª¨ë¸ ì¼ì‹œ ë¹„í™œì„±í™”
export DISABLE_HEAVY_MODELS=true
systemctl restart fastapi-app

# 3. ìºì‹œ ì›Œë°ì—…
python scripts/cache_warmup.py --stations all
```

#### ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€
```bash
# 1. ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
python -m memory_profiler app/main.py

# 2. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
curl -X POST http://localhost:8000/api/admin/gc-collect

# 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ì¬ì‹œì‘ (ì„ì‹œ ì¡°ì¹˜)
systemctl restart fastapi-app
```

#### ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ
- **ë³‘ë ¬ ì²˜ë¦¬**: 3-5ë°° ì†ë„ í–¥ìƒ
- **ìºì‹±**: 2-3ë°° ì†ë„ í–¥ìƒ  
- **ë©”ëª¨ë¦¬ ìµœì í™”**: 40% ë©”ëª¨ë¦¬ ì ˆì•½
- **ì „ì²´ ìµœì í™”**: ì‘ë‹µ ì‹œê°„ 1-2ì´ˆ â†’ 0.3-0.5ì´ˆ

---

## ğŸ“Š ì˜ˆì¸¡ ì‹ ë¢°ë„ í‰ê°€ ì‹œìŠ¤í…œ

### ì‹ ë¢°ë„ í‰ê°€ ê°œìš”
EV ì¶©ì „ì†Œ ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡ì˜ ì •í™•ì„±ê³¼ ì‹ ë¢°ì„±ì„ í‰ê°€í•˜ëŠ” ë‹¤ì¸µì  ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### 1. **ê°œë³„ ëª¨ë¸ ì‹ ë¢°ë„ (Model-Level Confidence)**

#### ê·¹ê°’ ì´ë¡  ëª¨ë¸
- **GEV (Generalized Extreme Value)**: `0.85` (ë§¤ìš° ë†’ìŒ)
  - ê·¹ê°’ ë¶„í¬ì˜ í†µê³„ì  ì´ë¡ ì— ê¸°ë°˜í•œ ë†’ì€ ì‹ ë¢°ë„
- **Gumbel ë¶„í¬**: `0.80` (ë†’ìŒ)  
  - ìµœëŒ“ê°’ ì˜ˆì¸¡ì— íŠ¹í™”ëœ ì•ˆì •ì  ì„±ëŠ¥

#### í†µê³„ì  ì¶”ë¡  ëª¨ë¸
- **ë¶„ìœ„ìˆ˜ íšŒê·€**: `0.75` (ì¤‘ê°„-ë†’ìŒ)
  - ë°ì´í„° ë¶„í¬ë¥¼ ê³ ë ¤í•œ ê²¬ê³ í•œ ì˜ˆì¸¡
- **ì»¤ë„ ë°€ë„ ì¶”ì •**: `0.85` (ë§¤ìš° ë†’ìŒ)
  - ë¹„ëª¨ìˆ˜ì  ì ‘ê·¼ìœ¼ë¡œ ìœ ì—°ì„± ì œê³µ

#### ë² ì´ì§€ì•ˆ ëª¨ë¸
- **ë² ì´ì§€ì•ˆ ì¶”ì •**: `0.75` (ì¤‘ê°„-ë†’ìŒ)
  - ì‚¬ì „ ì§€ì‹ê³¼ ë°ì´í„°ë¥¼ ê²°í•©í•œ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
- **ë¶€íŠ¸ìŠ¤íŠ¸ë©**: `0.80` (ë†’ìŒ)
  - í†µê³„ì  ì¬í‘œë³¸ ê¸°ë²•ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´

#### ì‹œê³„ì—´ ëª¨ë¸
- **ì§€ìˆ˜ í‰í™œë²•**: `0.70` (ì¤‘ê°„)
  - ì‹œê°„ì  íŒ¨í„´ ë°˜ì˜, ë‹¨ê¸° ì˜ˆì¸¡ì— ìœ ë¦¬
- **ì¶”ì„¸ ë¶„ì„**: `0.70` (ì¤‘ê°„)
  - ì¥ê¸° ê²½í–¥ì„± íŒŒì•…ì— íš¨ê³¼ì 

#### ê°•ê±´ í†µê³„ëŸ‰ ëª¨ë¸
- **MAD ê¸°ë°˜**: `0.75` (ì¤‘ê°„-ë†’ìŒ)
  - ì´ìƒì¹˜ì— ê°•í•œ ì¤‘ìœ„ìˆ˜ ì ˆëŒ€í¸ì°¨ ì‚¬ìš©

### 2. **ì•™ìƒë¸” ì‹ ë¢°ë„ (Ensemble Confidence)**

#### ê°€ì¤‘ í‰ê·  ë°©ì‹
```python
# ê° ëª¨ë¸ì˜ ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
total_confidence = sum(model.confidence_score for model in models)
weight = model.confidence_score / total_confidence

# ìµœì¢… ì˜ˆì¸¡ê°’
final_prediction = sum(model.prediction * weight for model, weight in zip(models, weights))
```

#### ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
- **í‘œì¤€í¸ì°¨ ê¸°ë°˜**: `np.std(predictions_array)`
- **ì‹ ë¢°êµ¬ê°„**: ê° ëª¨ë¸ë³„ 95% ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
- **ìµœì¢… ë¶ˆí™•ì‹¤ì„±**: ëª¨ë¸ ê°„ ì˜ˆì¸¡ê°’ ë¶„ì‚°ìœ¼ë¡œ ê³„ì‚°

### 3. **ì‹œìŠ¤í…œ ë ˆë²¨ ì‹ ë¢°ë„ (System-Level Confidence)**

#### ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ì‹ ë¢°ë„
```python
# ë°ì´í„° ê°œìˆ˜ì— ë”°ë¥¸ ê¸°ë³¸ ì‹ ë¢°ë„
base_confidence = min(0.95, max(0.4, data_count / 1000))

# ëª¨ë¸ ë¶ˆí™•ì‹¤ì„± ë°˜ì˜
model_confidence = max(0.4, 1.0 - (uncertainty / 100.0))

# ìµœì¢… ì‹ ë¢°ë„ (í‰ê· )
final_confidence = min(0.95, (base_confidence + model_confidence) / 2)
```

#### ì‹ ë¢°ë„ ë“±ê¸‰ ì²´ê³„
- **ë§¤ìš° ë†’ìŒ** (`0.90-0.95`): 1ë…„ ì´ìƒ í’ë¶€í•œ ë°ì´í„°, ì¼ê´€ëœ íŒ¨í„´
- **ë†’ìŒ** (`0.80-0.89`): 6ê°œì›” ì´ìƒ ë°ì´í„°, ì•ˆì •ì  íŒ¨í„´  
- **ì¤‘ê°„** (`0.70-0.79`): 3ê°œì›” ì´ìƒ ë°ì´í„°, ì¼ë¶€ ë³€ë™ì„±
- **ë‚®ìŒ** (`0.60-0.69`): 1ê°œì›” ì´ìƒ ë°ì´í„°, ë†’ì€ ë³€ë™ì„±
- **ë§¤ìš° ë‚®ìŒ** (`0.40-0.59`): ë¶€ì¡±í•œ ë°ì´í„°, ë¶ˆì•ˆì •í•œ íŒ¨í„´

### 4. **ì‹ ë¢°ë„ ê²€ì¦ ë°©ë²•**

#### ì¼ì¼ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
```bash
# 1. ì‹ ë¢°ë„ ì ìˆ˜ í™•ì¸
curl http://localhost:8000/api/stations/STATION_ID/predict | jq '.confidence'

# 2. ëª¨ë¸ë³„ ì‹ ë¢°ë„ ë¶„ì„
python -c "
from app.services.station_service import StationService
service = StationService()
result = service.get_prediction_analysis('STATION_ID')
for model in result['models']:
    print(f'{model[\"name\"]}: {model[\"confidence\"]:.2f}')
"

# 3. ë¶ˆí™•ì‹¤ì„± ì§€í‘œ í™•ì¸
curl http://localhost:8000/api/stations/STATION_ID/predict | jq '.advanced_model_prediction.uncertainty'
```

#### ì£¼ê°„ ì‹ ë¢°ë„ ë¶„ì„
```bash
# ì‹ ë¢°ë„ ì¶”ì´ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
cat > confidence_analysis.py << 'EOF'
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

def analyze_confidence_trends(station_id, days=7):
    """ìµœê·¼ 7ì¼ê°„ ì‹ ë¢°ë„ ì¶”ì´ ë¶„ì„"""
    
    # ë¡œê·¸ì—ì„œ ì‹ ë¢°ë„ ë°ì´í„° ì¶”ì¶œ
    confidence_data = []
    
    for i in range(days):
        date = datetime.now() - timedelta(days=i)
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë¡œê·¸ íŒŒì¼ì´ë‚˜ DBì—ì„œ ë°ì´í„° ì¶”ì¶œ
        # confidence_data.append({
        #     'date': date,
        #     'confidence': get_daily_confidence(station_id, date)
        # })
    
    df = pd.DataFrame(confidence_data)
    
    print(f"Station {station_id} - ì‹ ë¢°ë„ ë¶„ì„")
    print(f"í‰ê·  ì‹ ë¢°ë„: {df['confidence'].mean():.3f}")
    print(f"ì‹ ë¢°ë„ í‘œì¤€í¸ì°¨: {df['confidence'].std():.3f}")
    print(f"ìµœì†Œ/ìµœëŒ€: {df['confidence'].min():.3f} / {df['confidence'].max():.3f}")
    
    return df

if __name__ == "__main__":
    analyze_confidence_trends("STATION_001")
EOF

python confidence_analysis.py
```

### 5. **ì‹ ë¢°ë„ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ê°€ì´ë“œ**

#### ë†’ì€ ì‹ ë¢°ë„ (0.8+)
- âœ… **ê¶Œê³ **: ì˜ˆì¸¡ê°’ì„ ê·¸ëŒ€ë¡œ ê³„ì•½ì „ë ¥ ì„¤ì •ì— í™œìš©
- âœ… **ìë™í™”**: ì‹œìŠ¤í…œ ìë™ ì¶”ì²œ í—ˆìš©
- âœ… **ì•Œë¦¼**: ì •ìƒ ìˆ˜ì¤€ì˜ ëª¨ë‹ˆí„°ë§

#### ì¤‘ê°„ ì‹ ë¢°ë„ (0.6-0.8)
- âš ï¸ **ì£¼ì˜**: ì˜ˆì¸¡ê°’ì— ì•ˆì „ ë§ˆì§„ ì¶”ê°€ ê³ ë ¤
- âš ï¸ **ê²€í† **: ì „ë¬¸ê°€ ê²€í†  ê¶Œì¥  
- âš ï¸ **ëª¨ë‹ˆí„°ë§**: ì£¼ê°„ ë‹¨ìœ„ ì„±ëŠ¥ ì ê²€

#### ë‚®ì€ ì‹ ë¢°ë„ (0.4-0.6)
- âŒ **ê²½ê³ **: ì˜ˆì¸¡ê°’ ì‚¬ìš© ì‹œ ì‹ ì¤‘í•œ íŒë‹¨ í•„ìš”
- âŒ **ìˆ˜ë™**: ìˆ˜ë™ ê²€ì¦ ë° ì¡°ì • ê¶Œì¥
- âŒ **ë¹ˆë²ˆ**: ì¼ì¼ ë‹¨ìœ„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### 6. **ì‹ ë¢°ë„ ê°œì„  ë°©ì•ˆ**

#### ë°ì´í„° í’ˆì§ˆ ê°œì„ 
```bash
# 1. ë°ì´í„° ì™„ì„±ë„ í™•ì¸
python -c "
import pandas as pd
from app.data.loader import ChargingDataLoader

loader = ChargingDataLoader('STATION_ID')
data = loader.load_historical_sessions(days=365)

print(f'ì „ì²´ ê¸°ê°„: {data.index.min()} ~ {data.index.max()}')
print(f'ê²°ì¸¡ì¹˜ ë¹„ìœ¨: {data.isnull().sum().sum() / len(data):.1%}')
print(f'ë°ì´í„° ë°€ë„: {len(data) / 365:.1f}ê±´/ì¼')
"

# 2. ì´ìƒì¹˜ ì œê±°
python scripts/clean_outliers.py --station-id STATION_ID --threshold 3.0

# 3. ë°ì´í„° ë³´ê°„
python scripts/interpolate_missing.py --station-id STATION_ID --method linear
```

#### ëª¨ë¸ íŒŒë¼ë¯¸í„° íŠœë‹
```bash
# 1. ê³„ì ˆì„± íŒŒë¼ë¯¸í„° ì¡°ì •
# app/prediction/engine.pyì—ì„œ ê³„ì ˆ ê°€ì¤‘ì¹˜ ìˆ˜ì •

# 2. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”
python scripts/optimize_ensemble.py --station-id STATION_ID

# 3. ì‹ ë¢°êµ¬ê°„ ì¬ì¡°ì •
python scripts/calibrate_confidence.py --validation-period 30
```

#### ì˜ˆì¸¡ êµ¬ê°„ ì¡°ì •
- **ë‹¨ê¸° ì˜ˆì¸¡** (1-7ì¼): ì§€ìˆ˜ í‰í™œë²• ê°€ì¤‘ì¹˜ ì¦ê°€
- **ì¤‘ê¸° ì˜ˆì¸¡** (1-3ê°œì›”): í†µê³„ì  ì¶”ë¡  ëª¨ë¸ ìš°ì„ 
- **ì¥ê¸° ì˜ˆì¸¡** (6ê°œì›”+): ê·¹ê°’ ì´ë¡  ëª¨ë¸ ì¤‘ì‹¬

### 7. **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**

#### ì—…ê³„ í‘œì¤€ ë¹„êµ
- **ì¼ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡**: ì‹ ë¢°ë„ 0.70-0.80
- **ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡**: ì‹ ë¢°ë„ 0.75-0.85  
- **EV ì¶©ì „ ì˜ˆì¸¡**: ì‹ ë¢°ë„ 0.70-0.90 (ë°ì´í„° í’ˆì§ˆ ì˜ì¡´)

#### ëª©í‘œ ì‹ ë¢°ë„ ì„¤ì •
- **ë‹¨ê¸°** (1-7ì¼): â‰¥ 0.85
- **ì¤‘ê¸°** (1-3ê°œì›”): â‰¥ 0.80  
- **ì¥ê¸°** (6ê°œì›”+): â‰¥ 0.75

### 8. **ë¬¸ì œ í•´ê²°**

#### ì‹ ë¢°ë„ ê¸‰ë½ ì‹œ
```bash
# 1. ìµœê·¼ ë°ì´í„° í’ˆì§ˆ í™•ì¸
python debug_station_data.py STATION_ID --recent-days 7

# 2. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¶„ì„  
python debug_model_performance.py STATION_ID

# 3. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì¬ê³„ì‚°
python scripts/recalibrate_ensemble.py STATION_ID
```

#### ì¼ê´€ì„± ì—†ëŠ” ì˜ˆì¸¡
```bash
# 1. ë°ì´í„° drift ê²€ì‚¬
python scripts/detect_data_drift.py STATION_ID

# 2. ê³„ì ˆì„± íŒ¨í„´ ë³€í™” í™•ì¸
python scripts/analyze_seasonality.py STATION_ID --years 2

# 3. ëª¨ë¸ ì¬í›ˆë ¨
python scripts/retrain_models.py STATION_ID --validation-split 0.2
```

---

*ğŸ“… ë¬¸ì„œ ìµœì¢… ì—…ë°ì´íŠ¸: 2025ë…„ 9ì›” 1ì¼*  